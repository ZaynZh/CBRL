{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from goal_task import GoalTask\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = 'Hebbian'\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"MoA_ESN_wandb\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = False\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    #Environment specific arguments\n",
    "    goal_change = False\n",
    "    test = False\n",
    "    initial_positions = None\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"GoalChange\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 120000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 3e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    buffer_size: int = 64\n",
    "    \"\"\"the replay memory buffer size\"\"\"\n",
    "    gamma: float = 0.95\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    tau: float = 0.05\n",
    "    \"\"\"target smoothing coefficient (default: 0.005)\"\"\"\n",
    "    batch_size: int = 64\n",
    "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "    policy_noise: float = 0\n",
    "    \"\"\"the scale of policy noise\"\"\"\n",
    "    exploration_noise: float = 0\n",
    "    \"\"\"the scale of exploration noise\"\"\"\n",
    "    learning_starts: int = 0\n",
    "    \"\"\"timestep to start learning\"\"\"\n",
    "    policy_frequency: int = 2\n",
    "    \"\"\"the frequency of training policy (delayed)\"\"\"\n",
    "    noise_clip: float = 0.5\n",
    "    \"\"\"noise clip parameter of the Target Policy Smoothing Regularization\"\"\"\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name, initial_position=None):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = GoalTask(initial_position = initial_position)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod() + np.prod(env.action_space.shape), 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod(), 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc_mu = nn.Linear(32, np.prod(env.action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "class ESNLayer(nn.Module):\n",
    "    def __init__(self, env, reservoir_size, spectral_radius=0.95, g=2.2, sparsity=0.1, hebb_lr=3e-4, device='cuda'):\n",
    "        super(ESNLayer, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.input_size = np.array(env.observation_space.shape).prod()\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.g = g\n",
    "        \n",
    "        # Input weights: weights are sampled from a uniform distribution over [-0.5,0.5]\n",
    "        self.W_in = torch.rand(reservoir_size, self.input_size, device=self.device) - 0.5\n",
    "        \n",
    "        # Reservoir weights: sparse random matrix\n",
    "        self.W = torch.randn(reservoir_size, reservoir_size, device=self.device)\n",
    "        self.W[torch.rand(reservoir_size, reservoir_size) > sparsity] = 0.0  # Sparsify\n",
    "        self.W_mask = torch.ones((reservoir_size, reservoir_size), device=self.device)\n",
    "        self.W_mask[self.W == 0] = 0\n",
    "        \n",
    "        # Scale the reservoir weights to have the desired spectral radius\n",
    "        _, eigenvalues = torch.linalg.eig(self.W)\n",
    "        max_eigenvalue = torch.max(torch.abs(eigenvalues))\n",
    "        self.W *= self.spectral_radius / max_eigenvalue\n",
    "\n",
    "        #Hebbian parameters\n",
    "        self.hebb_trace = torch.zeros((256,256), device=self.device)\n",
    "        self.hebb_step = 0\n",
    "        self.past_activation = torch.zeros((256,256), device=self.device)\n",
    "        self.delta_W = torch.zeros((256,256), device=self.device)\n",
    "        self.hebb_lr = hebb_lr\n",
    "        \n",
    "        # The output weights, to be learned during training\n",
    "        self.W_out = nn.Linear((reservoir_size+self.input_size), env.action_space.shape[0], device=self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, u, hidden_state):\n",
    "        # Compute the new reservoir state with leaky integration\n",
    "        with torch.no_grad():\n",
    "            reservoir_output = torch.matmul(self.W, hidden_state.T) + 0.2 * torch.matmul(self.hebb_trace, hidden_state.T)\n",
    "            next_hidden_state = torch.matmul(self.W_in, u.T) + self.g * reservoir_output\n",
    "            next_hidden_state = torch.tanh(next_hidden_state)\n",
    "        self.next_hidden_state = next_hidden_state\n",
    "        self.hidden_state = hidden_state\n",
    "        # Append output prediction (linear transformation of the reservoir state)\n",
    "        output = torch.tanh(self.W_out(torch.concat([next_hidden_state.T, u],dim=-1)))\n",
    "\n",
    "        # Stack outputs over time and return\n",
    "        return output\n",
    "    \n",
    "    def hebbtrace_update(self, hidden_state):\n",
    "        with torch.no_grad():\n",
    "            activations = (torch.matmul(self.W, hidden_state.T) + 0.2 * torch.matmul(self.hebb_trace, hidden_state.T)).T  # Shape: (batch_size, num_neurons)\n",
    "            \n",
    "            # Calculate weight updates for each sample in the batch\n",
    "            batch_size = hidden_state.size(0)\n",
    "            weight_updates = torch.zeros_like(self.hebb_trace)  # Initialize weight updates to zero\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # For each sample, calculate the Oja update rule\n",
    "                y = activations[i]  # Shape: (num_neurons,)\n",
    "                # Number of elements in the top 10%\n",
    "                k = int(y.numel() * 0.1)\n",
    "\n",
    "                # Get the threshold value for the top 10%\n",
    "                top_10_percent_values, _ = torch.topk(y.flatten(), k)\n",
    "                threshold = top_10_percent_values[-1]  # The smallest value in the top 10%\n",
    "\n",
    "                # Create a mask where values are above or equal to the threshold\n",
    "                mask = y >= threshold\n",
    "                y = y * mask\n",
    "                x_i = hidden_state[i].unsqueeze(0)  # Shape: (1, input_size)\n",
    "                \n",
    "                # Compute the outer product update based on Oja's rule\n",
    "                # weight_updates_i = learning_rate * (y * x_i - y^2 * weights)\n",
    "                weight_updates += self.hebb_lr * (\n",
    "                    torch.matmul(y.unsqueeze(1), x_i) - (y ** 2).unsqueeze(1) * self.hebb_trace\n",
    "                )\n",
    "\n",
    "            # Average the updates over the batch and apply to weights\n",
    "            self.hebb_trace.data += weight_updates / batch_size\n",
    "            self.hebb_trace = self.hebb_trace * self.W_mask\n",
    "            _, eigenvalues = torch.linalg.eig(self.hebb_trace)\n",
    "            max_eigenvalue = torch.max(torch.abs(eigenvalues))\n",
    "            self.hebb_step *= self.spectral_radius / max_eigenvalue\n",
    "            \n",
    "    \n",
    "    def get_hiddenstate(self):\n",
    "        return self.hidden_state, self.next_hidden_state\n",
    "    \n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset the hidden state of the ESN.\"\"\"\n",
    "        self.hiddenstate = torch.zeros(self.reservoir_size)\n",
    "    \n",
    "\n",
    "\n",
    "def inference(actor, hidden_state, args, global_step):\n",
    "    trajectory = []\n",
    "    for initial_postion in args.initial_positions:\n",
    "        inference_env = make_env(args.env_id, args.seed, 0, args.capture_video, run_name, initial_position=initial_postion)()\n",
    "        # Implement goal change by input different goal area\n",
    "        if global_step < args.total_timesteps/2:\n",
    "            inference_env.change_goal((15,10))\n",
    "        elif global_step >= args.total_timesteps/2:\n",
    "            inference_env.change_goal((5,10))\n",
    "        obs, _ = inference_env.reset() \n",
    "        hidden_state = hidden_state\n",
    "        inference_actor = actor\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                action = inference_actor(torch.Tensor(obs).to(device), torch.Tensor(hidden_state).to(device))\n",
    "                action = action.cpu().numpy()\n",
    "                next_obs, reward, termination, truncated, info = inference_env.step(action)\n",
    "                _, next_hidden_state = inference_actor.get_hiddenstate()\n",
    "                if termination:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "                hidden_state = next_hidden_state\n",
    "        trajectory.append(inference_env.get_trajectory())\n",
    "        inference_env.close()\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "\n",
    "if sb3.__version__ < \"2.0\":\n",
    "    raise ValueError(\n",
    "            \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
    "poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "args = Args()\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env setup\n",
    "torch.manual_seed(args.seed)\n",
    "envs = make_env(args.env_id, args.seed, 0, args.capture_video, run_name)()\n",
    "envs.change_goal((15,10))\n",
    "reservoir_size = 256\n",
    "input_space = spaces.Box(low=-1, high=1, shape=(envs.observation_space.shape[0]+reservoir_size,), dtype=np.float32)\n",
    "\n",
    "actor =  ESNLayer(envs, 256).to(device)\n",
    "qf1 = QNetwork(envs).to(device)\n",
    "qf2 = QNetwork(envs).to(device)\n",
    "qf1_target = QNetwork(envs).to(device)\n",
    "qf2_target = QNetwork(envs).to(device)\n",
    "target_actor = ESNLayer(envs, 256).to(device)\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "qf2_target.load_state_dict(qf2.state_dict())\n",
    "q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.learning_rate)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.learning_rate)\n",
    "\n",
    "envs.observation_space.dtype = np.float32\n",
    "\n",
    "\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    input_space,\n",
    "    envs.action_space,\n",
    "    device,\n",
    "    handle_timeout_termination=False,\n",
    ")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, _ = envs.reset(seed=args.seed)\n",
    "hidden_state = np.zeros((reservoir_size,))\n",
    "input = np.concat((obs,hidden_state))\n",
    "for i in range(400):\n",
    "    obs = input[:envs.observation_space.shape[0]]\n",
    "    hidden_state = input[envs.observation_space.shape[0]:]\n",
    "    with torch.no_grad():\n",
    "        actions = actor(torch.Tensor(obs).to(device), torch.Tensor(hidden_state).to(device))\n",
    "        actions = actions.cpu().numpy()\n",
    "    hidden_state, next_hidden_state = actor.get_hiddenstate()\n",
    "    hidden_state, next_hidden_state = hidden_state.cpu(), next_hidden_state.cpu()\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "    input = input = np.concat((obs,hidden_state))\n",
    "    next_input = np.concat((next_obs, next_hidden_state))\n",
    "    rb.add(input, next_input, actions, rewards, terminations, infos)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=199, episodic_return=-0.04\n",
      "global_step=399, episodic_return=-0.22000000000000006\n",
      "global_step=599, episodic_return=-0.6700000000000004\n",
      "global_step=799, episodic_return=-0.6100000000000003\n",
      "global_step=853, episodic_return=1\n",
      "global_step=1053, episodic_return=-0.7400000000000004\n",
      "global_step=1253, episodic_return=-0.7000000000000004\n",
      "global_step=1453, episodic_return=-0.4400000000000002\n",
      "global_step=1537, episodic_return=0.97\n",
      "global_step=1737, episodic_return=-0.18000000000000002\n",
      "global_step=1937, episodic_return=-0.3300000000000001\n",
      "global_step=1957, episodic_return=1\n",
      "global_step=2157, episodic_return=-0.4200000000000002\n",
      "global_step=2357, episodic_return=0\n",
      "global_step=2557, episodic_return=-0.13999999999999999\n",
      "global_step=2679, episodic_return=0.91\n",
      "global_step=2879, episodic_return=-0.16\n",
      "global_step=2999, episodic_return=0.95\n",
      "global_step=3133, episodic_return=0.96\n",
      "global_step=3167, episodic_return=1\n",
      "global_step=3367, episodic_return=-0.26000000000000006\n",
      "global_step=3567, episodic_return=-0.5300000000000002\n",
      "global_step=3767, episodic_return=-0.47000000000000025\n",
      "global_step=3967, episodic_return=-0.36000000000000015\n",
      "global_step=4058, episodic_return=1\n",
      "global_step=4184, episodic_return=0.9\n",
      "global_step=4384, episodic_return=-0.16\n",
      "global_step=4584, episodic_return=-0.18000000000000002\n",
      "global_step=4684, episodic_return=1\n",
      "global_step=4708, episodic_return=1\n",
      "global_step=4778, episodic_return=1\n",
      "global_step=4913, episodic_return=0.96\n",
      "global_step=5113, episodic_return=-0.08\n",
      "global_step=5144, episodic_return=1\n",
      "global_step=5220, episodic_return=0.69\n",
      "global_step=5420, episodic_return=-0.07\n",
      "global_step=5446, episodic_return=1\n",
      "global_step=5491, episodic_return=0.96\n",
      "global_step=5691, episodic_return=-0.2800000000000001\n",
      "global_step=5768, episodic_return=0.91\n",
      "global_step=5798, episodic_return=0.99\n",
      "global_step=5998, episodic_return=-0.13999999999999999\n",
      "global_step=6056, episodic_return=1\n",
      "global_step=6137, episodic_return=1\n",
      "global_step=6337, episodic_return=-0.08\n",
      "global_step=6516, episodic_return=1\n",
      "global_step=6611, episodic_return=1\n",
      "global_step=6751, episodic_return=1\n",
      "global_step=6807, episodic_return=1\n",
      "global_step=6876, episodic_return=1\n",
      "global_step=7076, episodic_return=-0.7000000000000004\n",
      "global_step=7186, episodic_return=0.99\n",
      "global_step=7386, episodic_return=-0.45000000000000023\n",
      "global_step=7458, episodic_return=0.99\n",
      "global_step=7511, episodic_return=0.99\n",
      "global_step=7553, episodic_return=0.95\n",
      "global_step=7688, episodic_return=0.75\n",
      "global_step=7764, episodic_return=0.73\n",
      "global_step=7840, episodic_return=0.83\n",
      "global_step=8029, episodic_return=0.5399999999999998\n",
      "global_step=8068, episodic_return=0.88\n",
      "global_step=8220, episodic_return=0.88\n",
      "global_step=8320, episodic_return=1\n",
      "global_step=8408, episodic_return=1\n",
      "global_step=8498, episodic_return=0.94\n",
      "global_step=8698, episodic_return=-0.45000000000000023\n",
      "global_step=8826, episodic_return=0.9299999999999999\n",
      "global_step=8998, episodic_return=0.94\n",
      "global_step=9063, episodic_return=0.96\n",
      "global_step=9076, episodic_return=1\n",
      "global_step=9265, episodic_return=0.91\n",
      "global_step=9363, episodic_return=1\n",
      "global_step=9403, episodic_return=0.97\n",
      "global_step=9512, episodic_return=0.95\n",
      "global_step=9631, episodic_return=0.94\n",
      "global_step=9766, episodic_return=0.98\n",
      "global_step=9802, episodic_return=0.99\n",
      "global_step=9862, episodic_return=0.97\n",
      "global_step=10062, episodic_return=-0.10999999999999999\n",
      "global_step=10146, episodic_return=0.99\n",
      "global_step=10189, episodic_return=1\n",
      "global_step=10224, episodic_return=1\n",
      "global_step=10298, episodic_return=0.98\n",
      "global_step=10411, episodic_return=0.94\n",
      "global_step=10467, episodic_return=1\n",
      "global_step=10499, episodic_return=1\n",
      "global_step=10574, episodic_return=0.99\n",
      "global_step=10634, episodic_return=0.98\n",
      "global_step=10670, episodic_return=1\n",
      "global_step=10694, episodic_return=1\n",
      "global_step=10773, episodic_return=1\n",
      "global_step=10841, episodic_return=1\n",
      "global_step=10857, episodic_return=1\n",
      "global_step=10908, episodic_return=0.97\n",
      "global_step=10992, episodic_return=0.9299999999999999\n",
      "global_step=11055, episodic_return=0.7999999999999999\n",
      "global_step=11255, episodic_return=-0.02\n",
      "global_step=11317, episodic_return=1\n",
      "global_step=11385, episodic_return=1\n",
      "global_step=11479, episodic_return=0.87\n",
      "global_step=11537, episodic_return=0.99\n",
      "global_step=11578, episodic_return=0.98\n",
      "global_step=11662, episodic_return=0.91\n",
      "global_step=11696, episodic_return=1\n",
      "global_step=11721, episodic_return=1\n",
      "global_step=11742, episodic_return=1\n",
      "global_step=11922, episodic_return=0.91\n",
      "global_step=11970, episodic_return=1\n",
      "global_step=12004, episodic_return=0.91\n",
      "global_step=12175, episodic_return=0.7799999999999999\n",
      "global_step=12204, episodic_return=1\n",
      "global_step=12282, episodic_return=1\n",
      "global_step=12315, episodic_return=1\n",
      "global_step=12340, episodic_return=0.98\n",
      "global_step=12379, episodic_return=0.89\n",
      "global_step=12414, episodic_return=1\n",
      "global_step=12482, episodic_return=1\n",
      "global_step=12533, episodic_return=0.9\n",
      "global_step=12584, episodic_return=1\n",
      "global_step=12595, episodic_return=1\n",
      "global_step=12622, episodic_return=0.98\n",
      "global_step=12682, episodic_return=0.97\n",
      "global_step=12725, episodic_return=1\n",
      "global_step=12738, episodic_return=1\n",
      "global_step=12777, episodic_return=1\n",
      "global_step=12812, episodic_return=0.98\n",
      "global_step=12918, episodic_return=0.88\n",
      "global_step=12974, episodic_return=0.97\n",
      "global_step=13091, episodic_return=0.94\n",
      "global_step=13151, episodic_return=0.98\n",
      "global_step=13213, episodic_return=1\n",
      "global_step=13231, episodic_return=1\n",
      "global_step=13287, episodic_return=1\n",
      "global_step=13354, episodic_return=0.83\n",
      "global_step=13461, episodic_return=0.91\n",
      "global_step=13600, episodic_return=1\n",
      "global_step=13647, episodic_return=0.96\n",
      "global_step=13713, episodic_return=1\n",
      "global_step=13780, episodic_return=0.96\n",
      "global_step=13843, episodic_return=0.96\n",
      "global_step=13954, episodic_return=0.99\n",
      "global_step=14027, episodic_return=1\n",
      "global_step=14063, episodic_return=0.98\n",
      "global_step=14109, episodic_return=0.89\n",
      "global_step=14185, episodic_return=0.92\n",
      "global_step=14245, episodic_return=0.96\n",
      "global_step=14288, episodic_return=1\n",
      "global_step=14373, episodic_return=1\n",
      "global_step=14398, episodic_return=0.99\n",
      "global_step=14416, episodic_return=0.97\n",
      "global_step=14447, episodic_return=1\n",
      "global_step=14485, episodic_return=0.96\n",
      "global_step=14538, episodic_return=0.95\n",
      "global_step=14583, episodic_return=1\n",
      "global_step=14649, episodic_return=0.9299999999999999\n",
      "global_step=14682, episodic_return=0.99\n",
      "global_step=14881, episodic_return=0.83\n",
      "global_step=14992, episodic_return=0.97\n",
      "global_step=15129, episodic_return=0.88\n",
      "global_step=15228, episodic_return=0.82\n",
      "global_step=15342, episodic_return=1\n",
      "global_step=15385, episodic_return=1\n",
      "global_step=15484, episodic_return=0.96\n",
      "global_step=15510, episodic_return=1\n",
      "global_step=15549, episodic_return=0.9299999999999999\n",
      "global_step=15668, episodic_return=1\n",
      "global_step=15747, episodic_return=0.86\n",
      "global_step=15786, episodic_return=0.96\n",
      "global_step=15809, episodic_return=0.96\n",
      "global_step=15879, episodic_return=0.99\n",
      "global_step=15896, episodic_return=1\n",
      "global_step=15928, episodic_return=1\n",
      "global_step=16038, episodic_return=1\n",
      "global_step=16071, episodic_return=1\n",
      "global_step=16134, episodic_return=0.95\n",
      "global_step=16192, episodic_return=0.97\n",
      "global_step=16218, episodic_return=1\n",
      "global_step=16276, episodic_return=1\n",
      "global_step=16363, episodic_return=1\n",
      "global_step=16410, episodic_return=0.9299999999999999\n",
      "global_step=16488, episodic_return=1\n",
      "global_step=16584, episodic_return=0.95\n",
      "global_step=16655, episodic_return=1\n",
      "global_step=16683, episodic_return=1\n",
      "global_step=16772, episodic_return=0.9299999999999999\n",
      "global_step=16828, episodic_return=1\n",
      "global_step=16861, episodic_return=1\n",
      "global_step=16878, episodic_return=0.95\n",
      "global_step=16942, episodic_return=0.95\n",
      "global_step=16997, episodic_return=1\n",
      "global_step=17012, episodic_return=1\n",
      "global_step=17046, episodic_return=1\n",
      "global_step=17075, episodic_return=1\n",
      "global_step=17155, episodic_return=0.99\n",
      "global_step=17206, episodic_return=1\n",
      "global_step=17243, episodic_return=1\n",
      "global_step=17295, episodic_return=1\n",
      "global_step=17417, episodic_return=0.99\n",
      "global_step=17435, episodic_return=1\n",
      "global_step=17532, episodic_return=0.84\n",
      "global_step=17594, episodic_return=0.9299999999999999\n",
      "global_step=17659, episodic_return=1\n",
      "global_step=17715, episodic_return=1\n",
      "global_step=17736, episodic_return=0.99\n",
      "global_step=17753, episodic_return=0.96\n",
      "global_step=17780, episodic_return=0.97\n",
      "global_step=17829, episodic_return=1\n",
      "global_step=17864, episodic_return=1\n",
      "global_step=17887, episodic_return=1\n",
      "global_step=17931, episodic_return=0.95\n",
      "global_step=17967, episodic_return=1\n",
      "global_step=18001, episodic_return=1\n",
      "global_step=18029, episodic_return=1\n",
      "global_step=18080, episodic_return=0.99\n",
      "global_step=18142, episodic_return=0.98\n",
      "global_step=18209, episodic_return=0.97\n",
      "global_step=18223, episodic_return=0.95\n",
      "global_step=18281, episodic_return=1\n",
      "global_step=18303, episodic_return=1\n",
      "global_step=18325, episodic_return=1\n",
      "global_step=18340, episodic_return=1\n",
      "global_step=18364, episodic_return=1\n",
      "global_step=18399, episodic_return=1\n",
      "global_step=18440, episodic_return=0.99\n",
      "global_step=18549, episodic_return=0.98\n",
      "global_step=18575, episodic_return=1\n",
      "global_step=18610, episodic_return=1\n",
      "global_step=18647, episodic_return=1\n",
      "global_step=18673, episodic_return=1\n",
      "global_step=18696, episodic_return=1\n",
      "global_step=18756, episodic_return=0.94\n",
      "global_step=18805, episodic_return=0.97\n",
      "global_step=18868, episodic_return=1\n",
      "global_step=18943, episodic_return=1\n",
      "global_step=18977, episodic_return=1\n",
      "global_step=19020, episodic_return=0.99\n",
      "global_step=19078, episodic_return=0.99\n",
      "global_step=19114, episodic_return=0.98\n",
      "global_step=19144, episodic_return=0.98\n",
      "global_step=19160, episodic_return=1\n",
      "global_step=19202, episodic_return=0.96\n",
      "global_step=19226, episodic_return=1\n",
      "global_step=19277, episodic_return=1\n",
      "global_step=19343, episodic_return=1\n",
      "global_step=19378, episodic_return=1\n",
      "global_step=19515, episodic_return=0.98\n",
      "global_step=19570, episodic_return=1\n",
      "global_step=19600, episodic_return=0.98\n",
      "global_step=19622, episodic_return=1\n",
      "global_step=19695, episodic_return=1\n",
      "global_step=19837, episodic_return=0.86\n",
      "global_step=19882, episodic_return=1\n",
      "global_step=19934, episodic_return=1\n",
      "global_step=19958, episodic_return=1\n",
      "global_step=19996, episodic_return=0.98\n",
      "global_step=20084, episodic_return=1\n",
      "global_step=20125, episodic_return=0.99\n",
      "global_step=20158, episodic_return=0.98\n",
      "global_step=20203, episodic_return=1\n",
      "global_step=20237, episodic_return=1\n",
      "global_step=20267, episodic_return=1\n",
      "global_step=20283, episodic_return=1\n",
      "global_step=20314, episodic_return=1\n",
      "global_step=20392, episodic_return=1\n",
      "global_step=20442, episodic_return=0.98\n",
      "global_step=20489, episodic_return=0.99\n",
      "global_step=20504, episodic_return=1\n",
      "global_step=20531, episodic_return=1\n",
      "global_step=20572, episodic_return=0.96\n",
      "global_step=20624, episodic_return=1\n",
      "global_step=20644, episodic_return=0.97\n",
      "global_step=20666, episodic_return=1\n",
      "global_step=20696, episodic_return=0.98\n",
      "global_step=20744, episodic_return=1\n",
      "global_step=20768, episodic_return=1\n",
      "global_step=20808, episodic_return=1\n",
      "global_step=20839, episodic_return=0.99\n",
      "global_step=20857, episodic_return=0.94\n",
      "global_step=20887, episodic_return=1\n",
      "global_step=20999, episodic_return=0.96\n",
      "global_step=21019, episodic_return=1\n",
      "global_step=21050, episodic_return=1\n",
      "global_step=21070, episodic_return=1\n",
      "global_step=21115, episodic_return=0.99\n",
      "global_step=21144, episodic_return=1\n",
      "global_step=21234, episodic_return=0.92\n",
      "global_step=21251, episodic_return=1\n",
      "global_step=21281, episodic_return=1\n",
      "global_step=21312, episodic_return=1\n",
      "global_step=21339, episodic_return=1\n",
      "global_step=21396, episodic_return=1\n",
      "global_step=21413, episodic_return=1\n",
      "global_step=21435, episodic_return=0.99\n",
      "global_step=21529, episodic_return=1\n",
      "global_step=21575, episodic_return=0.99\n",
      "global_step=21626, episodic_return=0.99\n",
      "global_step=21659, episodic_return=1\n",
      "global_step=21693, episodic_return=0.99\n",
      "global_step=21728, episodic_return=1\n",
      "global_step=21760, episodic_return=0.99\n",
      "global_step=21802, episodic_return=1\n",
      "global_step=21853, episodic_return=1\n",
      "global_step=21885, episodic_return=0.98\n",
      "global_step=21941, episodic_return=1\n",
      "global_step=21959, episodic_return=0.99\n",
      "global_step=21987, episodic_return=1\n",
      "global_step=22082, episodic_return=0.98\n",
      "global_step=22104, episodic_return=1\n",
      "global_step=22142, episodic_return=0.96\n",
      "global_step=22195, episodic_return=0.96\n",
      "global_step=22213, episodic_return=1\n",
      "global_step=22306, episodic_return=0.97\n",
      "global_step=22329, episodic_return=1\n",
      "global_step=22377, episodic_return=0.88\n",
      "global_step=22453, episodic_return=1\n",
      "global_step=22476, episodic_return=1\n",
      "global_step=22511, episodic_return=1\n",
      "global_step=22525, episodic_return=0.99\n",
      "global_step=22542, episodic_return=0.99\n",
      "global_step=22595, episodic_return=0.94\n",
      "global_step=22667, episodic_return=1\n",
      "global_step=22711, episodic_return=0.92\n",
      "global_step=22800, episodic_return=1\n",
      "global_step=22828, episodic_return=0.97\n",
      "global_step=22861, episodic_return=1\n",
      "global_step=22894, episodic_return=1\n",
      "global_step=22953, episodic_return=0.9\n",
      "global_step=22984, episodic_return=1\n",
      "global_step=23010, episodic_return=1\n",
      "global_step=23060, episodic_return=1\n",
      "global_step=23078, episodic_return=1\n",
      "global_step=23152, episodic_return=0.95\n",
      "global_step=23179, episodic_return=0.99\n",
      "global_step=23206, episodic_return=1\n",
      "global_step=23227, episodic_return=1\n",
      "global_step=23253, episodic_return=1\n",
      "global_step=23300, episodic_return=0.99\n",
      "global_step=23325, episodic_return=1\n",
      "global_step=23348, episodic_return=1\n",
      "global_step=23367, episodic_return=1\n",
      "global_step=23414, episodic_return=1\n",
      "global_step=23456, episodic_return=1\n",
      "global_step=23492, episodic_return=0.98\n",
      "global_step=23534, episodic_return=0.9299999999999999\n",
      "global_step=23554, episodic_return=1\n",
      "global_step=23577, episodic_return=1\n",
      "global_step=23600, episodic_return=0.99\n",
      "global_step=23671, episodic_return=1\n",
      "global_step=23717, episodic_return=1\n",
      "global_step=23776, episodic_return=1\n",
      "global_step=23797, episodic_return=1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m actor_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     89\u001b[0m actor_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 90\u001b[0m \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhebbtrace_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# update the target network\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param, target_param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(actor\u001b[38;5;241m.\u001b[39mparameters(), target_actor\u001b[38;5;241m.\u001b[39mparameters()):\n",
      "Cell \u001b[1;32mIn[44], line 209\u001b[0m, in \u001b[0;36mESNLayer.hebbtrace_update\u001b[1;34m(self, hidden_state)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhebb_trace\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m weight_updates \u001b[38;5;241m/\u001b[39m batch_size\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhebb_trace \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhebb_trace \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_mask\n\u001b[1;32m--> 209\u001b[0m _, eigenvalues \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meig\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhebb_trace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m max_eigenvalue \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39mabs(eigenvalues))\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhebb_step \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectral_radius \u001b[38;5;241m/\u001b[39m max_eigenvalue\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRY NOT TO MODIFY: start the game\n",
    "obs, _ = envs.reset(seed=args.seed)\n",
    "hidden_state = np.zeros((reservoir_size,))\n",
    "input = np.concat((obs,hidden_state))\n",
    "all_trajectory = {}\n",
    "for global_step in range(args.total_timesteps):\n",
    "    # CHANGE GOAL\n",
    "    if global_step >= args.total_timesteps/2:\n",
    "        envs.change_goal((5,10))\n",
    "\n",
    "    # SAVE MODEL: save model per 2000 global steps\n",
    "    # model_dir = f\"runs/{run_name}/models\"\n",
    "    # os.makedirs(model_dir, exist_ok=True)\n",
    "    # if global_step % 2000 == 0 or global_step == args.total_timesteps:\n",
    "    #     model_path = f\"runs/{run_name}/models/{global_step}steps.cleanrl_model\"\n",
    "    #     torch.save(actor.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    obs = input[:envs.observation_space.shape[0]]\n",
    "    hidden_state = input[envs.observation_space.shape[0]:]\n",
    "\n",
    "    # INFERENCE:\n",
    "    if global_step % 8000 == 0 or global_step == args.total_timesteps-1:\n",
    "        args.initial_positions = [(2,2), (2,10), (2,18), (10,2), (10,18), (18,2), (18,10), (18,18)]\n",
    "        trajectory = inference(actor, hidden_state, args, global_step)\n",
    "        all_trajectory[f'{global_step}'] = trajectory\n",
    "\n",
    "\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    if global_step < args.learning_starts:\n",
    "        actions = envs.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            actions = actor(torch.Tensor(obs).to(device), torch.Tensor(hidden_state).to(device))\n",
    "            actions = actions.cpu().numpy().clip(envs.action_space.low, envs.action_space.high)\n",
    "            hidden_state, next_hidden_state = actor.get_hiddenstate()\n",
    "            hidden_state, next_hidden_state = hidden_state.cpu(), next_hidden_state.cpu()\n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "    input = input = np.concat((obs,hidden_state))\n",
    "    next_input = np.concat((next_obs, next_hidden_state))\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    if \"final_info\" in infos or terminations:\n",
    "        print(f\"global_step={global_step}, episodic_return={infos['episode']['r']}\")\n",
    "        writer.add_scalar(\"charts/episodic_return\", infos[\"episode\"][\"r\"], global_step)\n",
    "        writer.add_scalar(\"charts/episodic_length\", infos[\"episode\"][\"l\"], global_step)     \n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "    next_obs = next_obs.copy()\n",
    "    if truncations:\n",
    "        real_next_obs = infos[\"final_observation\"]\n",
    "    rb.add(input, next_input, actions, rewards, terminations, infos)\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    input = next_input\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_observations = data.next_observations[:,:envs.observation_space.shape[0]]\n",
    "            next_hidden_states = data.next_observations[:,envs.observation_space.shape[0]:]\n",
    "            observations = data.observations[:,:envs.observation_space.shape[0]]\n",
    "            hidden_states = data.observations[:,envs.observation_space.shape[0]:]\n",
    "            next_state_actions = target_actor(next_observations, next_hidden_states).clamp(\n",
    "                envs.action_space.low[0], envs.action_space.high[0]\n",
    "            )\n",
    "            qf1_next_target = qf1_target(next_observations, next_state_actions)\n",
    "            qf2_next_target = qf2_target(next_observations, next_state_actions)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target)\n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)\n",
    "\n",
    "        qf1_a_values = qf1(observations, data.actions).view(-1)\n",
    "        qf2_a_values = qf2(observations, data.actions).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "        qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        # optimize the model\n",
    "        q_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        if global_step % args.policy_frequency == 0:\n",
    "            actor_loss = -qf1(observations, actor(observations, hidden_states)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            actor.hebbtrace_update(hidden_states)\n",
    "\n",
    "            # update the target network\n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "envs.close()\n",
    "writer.close()\n",
    "\n",
    "with open(f\"runs/{run_name}/all_trajectory.pkl\", \"wb\") as file:\n",
    "    pickle.dump(all_trajectory, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig_dir = f'runs/{run_name}/figures'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "for trajectories in all_trajectory:\n",
    "    step = int(trajectories)\n",
    "    trajectories = all_trajectory[f'{trajectories}']\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(trajectories)))\n",
    "    for i, (trajectory, color) in enumerate(zip(trajectories, colors)):\n",
    "        ax.plot(trajectory[:, 0], trajectory[:, 1], color=color)\n",
    "        ax.scatter(trajectory[:, 0], trajectory[:, 1], color=color, s=10)\n",
    "    if step < args.total_timesteps/2:\n",
    "        circle = plt.Circle((15, 10), radius=2, color='black', fill=False, linestyle='-', linewidth=2)\n",
    "    elif step >= args.total_timesteps/2:\n",
    "        circle = plt.Circle((5, 10), radius=2, color='black', fill=False, linestyle='-', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    plt.xlim(0, 20)\n",
    "    plt.ylim(0, 20)\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.title('Agent Trajectory in 2D Environment')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{fig_dir}/{step}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM SEEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "\n",
    "for i in range(10):\n",
    "    if sb3.__version__ < \"2.0\":\n",
    "        raise ValueError(\n",
    "                \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
    "    poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
    "    \"\"\"\n",
    "            )\n",
    "\n",
    "    args = Args()\n",
    "    args.seed = random.randint(0, 2**32 - 1)\n",
    "    args.exp_name = 'HebbianSpectral'\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    torch.manual_seed(args.seed)\n",
    "    envs = make_env(args.env_id, args.seed, 0, args.capture_video, run_name)()\n",
    "    envs.change_goal((15,10))\n",
    "    reservoir_size = 256\n",
    "    input_space = spaces.Box(low=-1, high=1, shape=(envs.observation_space.shape[0]+reservoir_size,), dtype=np.float32)\n",
    "\n",
    "    actor =  ESNLayer(envs, 256).to(device)\n",
    "    qf1 = QNetwork(envs).to(device)\n",
    "    qf2 = QNetwork(envs).to(device)\n",
    "    qf1_target = QNetwork(envs).to(device)\n",
    "    qf2_target = QNetwork(envs).to(device)\n",
    "    target_actor = ESNLayer(envs, 256).to(device)\n",
    "    target_actor.load_state_dict(actor.state_dict())\n",
    "    qf1_target.load_state_dict(qf1.state_dict())\n",
    "    qf2_target.load_state_dict(qf2.state_dict())\n",
    "    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.learning_rate)\n",
    "    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.learning_rate)\n",
    "\n",
    "    envs.observation_space.dtype = np.float32\n",
    "\n",
    "\n",
    "\n",
    "    rb = ReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        input_space,\n",
    "        envs.action_space,\n",
    "        device,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    obs, _ = envs.reset(seed=args.seed)\n",
    "    hidden_state = np.zeros((reservoir_size,))\n",
    "    input = np.concat((obs,hidden_state))\n",
    "    all_trajectory = {}\n",
    "    for global_step in range(args.total_timesteps):\n",
    "        # CHANGE GOAL\n",
    "        if global_step >= args.total_timesteps/2:\n",
    "            envs.change_goal((5,10))\n",
    "\n",
    "        # SAVE MODEL: save model per 2000 global steps\n",
    "        # model_dir = f\"runs/{run_name}/models\"\n",
    "        # os.makedirs(model_dir, exist_ok=True)\n",
    "        # if global_step % 2000 == 0 or global_step == args.total_timesteps:\n",
    "        #     model_path = f\"runs/{run_name}/models/{global_step}steps.cleanrl_model\"\n",
    "        #     torch.save(actor.state_dict(), model_path)\n",
    "\n",
    "\n",
    "        obs = input[:envs.observation_space.shape[0]]\n",
    "        hidden_state = input[envs.observation_space.shape[0]:]\n",
    "\n",
    "        # INFERENCE:\n",
    "        if global_step % 8000 == 0 or global_step == args.total_timesteps-1:\n",
    "            args.initial_positions = [(2,2), (2,10), (2,18), (10,2), (10,18), (18,2), (18,10), (18,18)]\n",
    "            trajectory = inference(actor, hidden_state, args, global_step)\n",
    "            all_trajectory[f'{global_step}'] = trajectory\n",
    "\n",
    "\n",
    "        # ALGO LOGIC: put action logic here\n",
    "        if global_step < args.learning_starts:\n",
    "            actions = envs.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                actions = actor(torch.Tensor(obs).to(device), torch.Tensor(hidden_state).to(device))\n",
    "                actions = actions.cpu().numpy().clip(envs.action_space.low, envs.action_space.high)\n",
    "                hidden_state, next_hidden_state = actor.get_hiddenstate()\n",
    "                hidden_state, next_hidden_state = hidden_state.cpu(), next_hidden_state.cpu()\n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "        input = input = np.concat((obs,hidden_state))\n",
    "        next_input = np.concat((next_obs, next_hidden_state))\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        if \"final_info\" in infos or terminations:\n",
    "            writer.add_scalar(\"charts/episodic_return\", infos[\"episode\"][\"r\"], global_step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", infos[\"episode\"][\"l\"], global_step)     \n",
    "\n",
    "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "        next_obs = next_obs.copy()\n",
    "        if truncations:\n",
    "            real_next_obs = infos[\"final_observation\"]\n",
    "        rb.add(input, next_input, actions, rewards, terminations, infos)\n",
    "\n",
    "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "        input = next_input\n",
    "\n",
    "        # ALGO LOGIC: training.\n",
    "        if global_step > args.learning_starts:\n",
    "            data = rb.sample(args.batch_size)\n",
    "            with torch.no_grad():\n",
    "                next_observations = data.next_observations[:,:envs.observation_space.shape[0]]\n",
    "                next_hidden_states = data.next_observations[:,envs.observation_space.shape[0]:]\n",
    "                observations = data.observations[:,:envs.observation_space.shape[0]]\n",
    "                hidden_states = data.observations[:,envs.observation_space.shape[0]:]\n",
    "                next_state_actions = target_actor(next_observations, next_hidden_states).clamp(\n",
    "                    envs.action_space.low[0], envs.action_space.high[0]\n",
    "                )\n",
    "                qf1_next_target = qf1_target(next_observations, next_state_actions)\n",
    "                qf2_next_target = qf2_target(next_observations, next_state_actions)\n",
    "                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target)\n",
    "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)\n",
    "\n",
    "            qf1_a_values = qf1(observations, data.actions).view(-1)\n",
    "            qf2_a_values = qf2(observations, data.actions).view(-1)\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "            qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "            # optimize the model\n",
    "            q_optimizer.zero_grad()\n",
    "            qf_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            if global_step % args.policy_frequency == 0:\n",
    "                actor_loss = -qf1(observations, actor(observations, hidden_states)).mean()\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "                actor.hebbtrace_update(hidden_states)\n",
    "\n",
    "                # update the target network\n",
    "                for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "            if global_step % 100 == 0:\n",
    "                writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "                writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "    with open(f\"runs/{run_name}/all_trajectory.pkl\", \"wb\") as file:\n",
    "        pickle.dump(all_trajectory, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
