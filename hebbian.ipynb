{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "import pickle\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from goal_task import GoalTask\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = 'Hebbian'\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"MoA_ESN_wandb\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = False\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    #Environment specific arguments\n",
    "    goal_change = False\n",
    "    test = False\n",
    "    initial_positions = None\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"GoalTask\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 60000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 3e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    buffer_size: int = 64\n",
    "    \"\"\"the replay memory buffer size\"\"\"\n",
    "    gamma: float = 0.95\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    tau: float = 0.05\n",
    "    \"\"\"target smoothing coefficient (default: 0.005)\"\"\"\n",
    "    batch_size: int = 64\n",
    "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "    policy_noise: float = 0\n",
    "    \"\"\"the scale of policy noise\"\"\"\n",
    "    exploration_noise: float = 0\n",
    "    \"\"\"the scale of exploration noise\"\"\"\n",
    "    learning_starts: int = 0\n",
    "    \"\"\"timestep to start learning\"\"\"\n",
    "    policy_frequency: int = 2\n",
    "    \"\"\"the frequency of training policy (delayed)\"\"\"\n",
    "    noise_clip: float = 0.5\n",
    "    \"\"\"noise clip parameter of the Target Policy Smoothing Regularization\"\"\"\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name, initial_position=None):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = GoalTask(initial_position = initial_position)\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod() + np.prod(env.action_space.shape), 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod(), 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc_mu = nn.Linear(32, np.prod(env.action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "class ESNLayer(nn.Module):\n",
    "    def __init__(self, env, reservoir_size, spectral_radius=0.95, g=2.2, sparsity=0.1, hebb_lr=3e-4, device='cuda'):\n",
    "        super(ESNLayer, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.input_size = np.array(env.observation_space.shape).prod()\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.g = g\n",
    "        \n",
    "        # Input weights: weights are sampled from a uniform distribution over [-0.5,0.5]\n",
    "        self.W_in = torch.rand(reservoir_size, self.input_size, device=self.device) - 0.5\n",
    "        \n",
    "        # Reservoir weights: sparse random matrix\n",
    "        self.W = torch.randn(reservoir_size, reservoir_size, device=self.device)\n",
    "        self.W[torch.rand(reservoir_size, reservoir_size) > sparsity] = 0.0  # Sparsify\n",
    "        self.W_mask = torch.ones((reservoir_size, reservoir_size), device=self.device)\n",
    "        self.W_mask[self.W == 0] = 0\n",
    "\n",
    "        # Scale the reservoir weights to have the desired spectral radius\n",
    "        _, eigenvalues = torch.linalg.eig(self.W)\n",
    "        max_eigenvalue = torch.max(torch.abs(eigenvalues))\n",
    "        self.W *= self.spectral_radius / max_eigenvalue\n",
    "        self.W = nn.Parameter(self.W)\n",
    "        # Define the weight change for the reservoir\n",
    "        self.hebb_trace = torch.zeros((256,256), device=self.device)\n",
    "        self.hebb_step = 0\n",
    "        self.past_activation = torch.zeros((256,256), device=self.device)\n",
    "        self.delta_W = torch.zeros((256,256), device=self.device)\n",
    "        self.hebb_lr =hebb_lr\n",
    "\n",
    "        # The output weights, to be learned during training\n",
    "        self.W_out = nn.Linear((reservoir_size+self.input_size), env.action_space.shape[0], device=self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, u, hidden_state):\n",
    "        # Compute the new reservoir state with leaky integration\n",
    "        with torch.no_grad():\n",
    "            reservoir_output = torch.matmul(self.W, hidden_state.T) + 0.2 * torch.matmul(self.hebb_trace, hidden_state.T)\n",
    "            next_hidden_state = torch.matmul(self.W_in, u.T) + self.g * reservoir_output\n",
    "            next_hidden_state = torch.tanh(next_hidden_state)\n",
    "        self.next_hidden_state = next_hidden_state\n",
    "        self.hidden_state = hidden_state\n",
    "        \n",
    "        # Append output prediction (linear transformation of the reservoir state)\n",
    "        output = torch.tanh(self.W_out(torch.concat([next_hidden_state.T, u],dim=-1)))\n",
    "\n",
    "        # Stack outputs over time and return\n",
    "        return output\n",
    "    \n",
    "    def hebbian_update(self, hidden_state):\n",
    "        with torch.no_grad():\n",
    "            activations = (torch.matmul(self.W, hidden_state.T) + 0.2 * torch.matmul(self.hebb_trace, hidden_state.T)).T  # Shape: (batch_size, num_neurons)\n",
    "            \n",
    "            # Calculate weight updates for each sample in the batch\n",
    "            batch_size = hidden_state.size(0)\n",
    "            weight_updates = torch.zeros_like(self.hebb_trace)  # Initialize weight updates to zero\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # For each sample, calculate the Oja update rule\n",
    "                y = activations[i]  # Shape: (num_neurons,)\n",
    "                x_i = hidden_state[i].unsqueeze(0)  # Shape: (1, input_size)\n",
    "                \n",
    "                # Compute the outer product update based on Oja's rule\n",
    "                # weight_updates_i = learning_rate * (y * x_i - y^2 * weights)\n",
    "                weight_updates += self.hebb_lr * (\n",
    "                    torch.matmul(y.unsqueeze(1), x_i) - (y ** 2).unsqueeze(1) * self.W\n",
    "                )\n",
    "\n",
    "            # Average the updates over the batch and apply to weights\n",
    "            self.W.data += weight_updates / batch_size\n",
    "            self.W = nn.Parameter(self.W * self.W_mask)\n",
    "    \n",
    "    def hebbtrace_update(self, hidden_state):\n",
    "        with torch.no_grad():\n",
    "            activations = (torch.matmul(self.W, hidden_state.T) + 0.2 * torch.matmul(self.hebb_trace, hidden_state.T)).T  # Shape: (batch_size, num_neurons)\n",
    "            \n",
    "            # Calculate weight updates for each sample in the batch\n",
    "            batch_size = hidden_state.size(0)\n",
    "            weight_updates = torch.zeros_like(self.hebb_trace)  # Initialize weight updates to zero\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                # For each sample, calculate the Oja update rule\n",
    "                y = activations[i]  # Shape: (num_neurons,)\n",
    "                # Number of elements in the top 10%\n",
    "                k = int(y.numel() * 0.1)\n",
    "\n",
    "                # Get the threshold value for the top 10%\n",
    "                top_10_percent_values, _ = torch.topk(y.flatten(), k)\n",
    "                threshold = top_10_percent_values[-1]  # The smallest value in the top 10%\n",
    "\n",
    "                # Create a mask where values are above or equal to the threshold\n",
    "                mask = y >= threshold\n",
    "                y = y * mask\n",
    "                x_i = hidden_state[i].unsqueeze(0)  # Shape: (1, input_size)\n",
    "                \n",
    "                # Compute the outer product update based on Oja's rule\n",
    "                # weight_updates_i = learning_rate * (y * x_i - y^2 * weights)\n",
    "                weight_updates += self.hebb_lr * (\n",
    "                    torch.matmul(y.unsqueeze(1), x_i) - (y ** 2).unsqueeze(1) * self.hebb_trace\n",
    "                )\n",
    "\n",
    "            # Average the updates over the batch and apply to weights\n",
    "            self.hebb_trace.data += weight_updates / batch_size\n",
    "            self.hebb_trace = self.hebb_trace * self.W_mask\n",
    "            _, eigenvalues = torch.linalg.eig(self.hebb_trace)\n",
    "            max_eigenvalue = torch.max(torch.abs(eigenvalues))\n",
    "            self.hebb_step *= self.spectral_radius / max_eigenvalue            \n",
    "        \n",
    "    def get_hiddenstate(self):\n",
    "        return self.hidden_state, self.next_hidden_state\n",
    "    \n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset the hidden state of the ESN.\"\"\"\n",
    "        self.hiddenstate = torch.zeros(self.reservoir_size)\n",
    "    \n",
    "\n",
    "\n",
    "def inference(actor, hidden_state, args):\n",
    "    trajectory = []\n",
    "    for initial_postion in args.initial_positions:\n",
    "        inference_env = make_env(args.env_id, args.seed, 0, args.capture_video, run_name, initial_position=initial_postion)()\n",
    "        obs, _ = inference_env.reset() \n",
    "        hidden_state = hidden_state\n",
    "        inference_actor = actor\n",
    "        while True:\n",
    "            with torch.no_grad():\n",
    "                action = inference_actor(torch.Tensor(obs).to(device), torch.Tensor(hidden_state).to(device))\n",
    "                action = action.cpu().numpy()\n",
    "                next_obs, reward, termination, truncated, info = inference_env.step(action)\n",
    "                _, next_hidden_state = inference_actor.get_hiddenstate()\n",
    "                if termination:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "                hidden_state = next_hidden_state\n",
    "        trajectory.append(inference_env.get_trajectory())\n",
    "        inference_env.close()\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "\n",
    "if sb3.__version__ < \"2.0\":\n",
    "    raise ValueError(\n",
    "            \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
    "poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "args = Args()\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env setup\n",
    "torch.manual_seed(args.seed)\n",
    "envs = make_env(args.env_id, args.seed, 0, args.capture_video, run_name)()\n",
    "reservoir_size = 256\n",
    "input_space = spaces.Box(low=-1, high=1, shape=(envs.observation_space.shape[0]+reservoir_size,), dtype=np.float32)\n",
    "\n",
    "actor =  ESNLayer(envs, 256).to(device)\n",
    "qf1 = QNetwork(envs).to(device)\n",
    "qf2 = QNetwork(envs).to(device)\n",
    "qf1_target = QNetwork(envs).to(device)\n",
    "qf2_target = QNetwork(envs).to(device)\n",
    "target_actor = ESNLayer(envs, 256).to(device)\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "qf2_target.load_state_dict(qf2.state_dict())\n",
    "q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.learning_rate)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.learning_rate)\n",
    "\n",
    "envs.observation_space.dtype = np.float32\n",
    "\n",
    "\n",
    "\n",
    "rb = ReplayBuffer(\n",
    "    args.buffer_size,\n",
    "    input_space,\n",
    "    envs.action_space,\n",
    "    device,\n",
    "    handle_timeout_termination=False,\n",
    ")\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat in method wrapper_CUDA_addmv_)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m rb\u001b[38;5;241m.\u001b[39msample(args\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 18\u001b[0m     \u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhebbian_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 183\u001b[0m, in \u001b[0;36mESNLayer.hebbian_update\u001b[1;34m(self, hidden_state)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhebbian_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 183\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_W \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhebb_lr \u001b[38;5;241m*\u001b[39m (torch\u001b[38;5;241m.\u001b[39mmatmul(hidden_state\u001b[38;5;241m.\u001b[39mT, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpast_activation)\n\u001b[0;32m    184\u001b[0m         k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_W\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m    185\u001b[0m         top_10_percent_values, _ \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta_W\u001b[38;5;241m.\u001b[39mflatten(), k)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat in method wrapper_CUDA_addmv_)"
     ]
    }
   ],
   "source": [
    "obs, _ = envs.reset(seed=args.seed)\n",
    "hidden_state = np.zeros((reservoir_size,))\n",
    "input = np.concat((obs,hidden_state))\n",
    "for i in range(400):\n",
    "    obs = input[:envs.observation_space.shape[0]]\n",
    "    hidden_state = input[envs.observation_space.shape[0]:]\n",
    "    with torch.no_grad():\n",
    "        actions = actor(torch.Tensor(obs).to(device), torch.Tensor(hidden_state).to(device))\n",
    "        actions = actions.cpu().numpy()\n",
    "    hidden_state, next_hidden_state = actor.get_hiddenstate()\n",
    "    hidden_state, next_hidden_state = hidden_state.cpu(), next_hidden_state.cpu()\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "    input = input = np.concat((obs,hidden_state))\n",
    "    next_input = np.concat((next_obs, next_hidden_state))\n",
    "    rb.add(input, next_input, actions, rewards, terminations, infos)\n",
    "    rb.sample(args.batch_size)\n",
    "    with torch.no_grad():\n",
    "        actor.hebbian_update(hidden_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global_step=199, episodic_return=-0.38000000000000017\n",
      "global_step=399, episodic_return=-0.26000000000000006\n",
      "global_step=599, episodic_return=-0.4000000000000002\n",
      "global_step=799, episodic_return=-0.17\n",
      "global_step=999, episodic_return=-0.11999999999999998\n",
      "global_step=1199, episodic_return=-0.5100000000000002\n",
      "global_step=1387, episodic_return=0.6399999999999999\n",
      "global_step=1529, episodic_return=0.99\n",
      "global_step=1729, episodic_return=-0.8600000000000005\n",
      "global_step=1929, episodic_return=-0.22000000000000006\n",
      "global_step=2129, episodic_return=-0.07\n",
      "global_step=2329, episodic_return=-0.22000000000000006\n",
      "global_step=2441, episodic_return=0.97\n",
      "global_step=2641, episodic_return=0\n",
      "global_step=2841, episodic_return=-0.15\n",
      "global_step=3041, episodic_return=-0.10999999999999999\n",
      "global_step=3241, episodic_return=-0.4300000000000002\n",
      "global_step=3377, episodic_return=0.92\n",
      "global_step=3577, episodic_return=-0.3200000000000001\n",
      "global_step=3777, episodic_return=0\n",
      "global_step=3880, episodic_return=1\n",
      "global_step=3976, episodic_return=0.97\n",
      "global_step=4176, episodic_return=-0.45000000000000023\n",
      "global_step=4376, episodic_return=-0.5800000000000003\n",
      "global_step=4576, episodic_return=-0.2800000000000001\n",
      "global_step=4776, episodic_return=-0.18000000000000002\n",
      "global_step=4815, episodic_return=1\n",
      "global_step=4921, episodic_return=0.86\n",
      "global_step=4999, episodic_return=1\n",
      "global_step=5078, episodic_return=1\n",
      "global_step=5192, episodic_return=0.98\n",
      "global_step=5333, episodic_return=0.96\n",
      "global_step=5531, episodic_return=1\n",
      "global_step=5614, episodic_return=0.99\n",
      "global_step=5775, episodic_return=1\n",
      "global_step=5975, episodic_return=-0.01\n",
      "global_step=6175, episodic_return=-0.11999999999999998\n",
      "global_step=6375, episodic_return=-0.12999999999999998\n",
      "global_step=6424, episodic_return=1\n",
      "global_step=6510, episodic_return=1\n",
      "global_step=6548, episodic_return=0.99\n",
      "global_step=6748, episodic_return=-0.18000000000000002\n",
      "global_step=6834, episodic_return=0.9\n",
      "global_step=7034, episodic_return=-0.09\n",
      "global_step=7234, episodic_return=-0.2800000000000001\n",
      "global_step=7385, episodic_return=0.98\n",
      "global_step=7442, episodic_return=0.96\n",
      "global_step=7549, episodic_return=0.99\n",
      "global_step=7639, episodic_return=0.99\n",
      "global_step=7659, episodic_return=1\n",
      "global_step=7707, episodic_return=0.99\n",
      "global_step=7779, episodic_return=1\n",
      "global_step=7979, episodic_return=-0.07\n",
      "global_step=8179, episodic_return=-0.10999999999999999\n",
      "global_step=8218, episodic_return=1\n",
      "global_step=8370, episodic_return=0.87\n",
      "global_step=8442, episodic_return=0.97\n",
      "global_step=8603, episodic_return=0.94\n",
      "global_step=8712, episodic_return=0.98\n",
      "global_step=8783, episodic_return=0.99\n",
      "global_step=8853, episodic_return=1\n",
      "global_step=8963, episodic_return=0.98\n",
      "global_step=8988, episodic_return=0.99\n",
      "global_step=9039, episodic_return=0.98\n",
      "global_step=9065, episodic_return=1\n",
      "global_step=9265, episodic_return=-0.03\n",
      "global_step=9297, episodic_return=0.97\n",
      "global_step=9497, episodic_return=-0.12999999999999998\n",
      "global_step=9697, episodic_return=-0.060000000000000005\n",
      "global_step=9749, episodic_return=1\n",
      "global_step=9803, episodic_return=1\n",
      "global_step=9850, episodic_return=0.99\n",
      "global_step=9999, episodic_return=1\n",
      "global_step=10072, episodic_return=1\n",
      "global_step=10147, episodic_return=1\n",
      "global_step=10347, episodic_return=-0.02\n",
      "global_step=10439, episodic_return=0.98\n",
      "global_step=10571, episodic_return=0.95\n",
      "global_step=10610, episodic_return=1\n",
      "global_step=10668, episodic_return=0.99\n",
      "global_step=10720, episodic_return=1\n",
      "global_step=10767, episodic_return=0.99\n",
      "global_step=10822, episodic_return=0.92\n",
      "global_step=10851, episodic_return=1\n",
      "global_step=10894, episodic_return=1\n",
      "global_step=10952, episodic_return=1\n",
      "global_step=11019, episodic_return=0.99\n",
      "global_step=11219, episodic_return=0\n",
      "global_step=11358, episodic_return=0.9\n",
      "global_step=11420, episodic_return=0.99\n",
      "global_step=11493, episodic_return=0.97\n",
      "global_step=11602, episodic_return=0.98\n",
      "global_step=11627, episodic_return=0.99\n",
      "global_step=11683, episodic_return=0.99\n",
      "global_step=11711, episodic_return=1\n",
      "global_step=11802, episodic_return=1\n",
      "global_step=11849, episodic_return=1\n",
      "global_step=12010, episodic_return=1\n",
      "global_step=12034, episodic_return=1\n",
      "global_step=12075, episodic_return=1\n",
      "global_step=12099, episodic_return=1\n",
      "global_step=12140, episodic_return=1\n",
      "global_step=12200, episodic_return=1\n",
      "global_step=12299, episodic_return=0.86\n",
      "global_step=12322, episodic_return=1\n",
      "global_step=12397, episodic_return=0.9299999999999999\n",
      "global_step=12539, episodic_return=0.98\n",
      "global_step=12739, episodic_return=-0.01\n",
      "global_step=12770, episodic_return=0.98\n",
      "global_step=12825, episodic_return=0.98\n",
      "global_step=12870, episodic_return=0.99\n",
      "global_step=12920, episodic_return=1\n",
      "global_step=12968, episodic_return=1\n",
      "global_step=12990, episodic_return=1\n",
      "global_step=13064, episodic_return=0.99\n",
      "global_step=13113, episodic_return=0.96\n",
      "global_step=13147, episodic_return=0.98\n",
      "global_step=13267, episodic_return=0.97\n",
      "global_step=13331, episodic_return=0.99\n",
      "global_step=13427, episodic_return=0.98\n",
      "global_step=13474, episodic_return=1\n",
      "global_step=13501, episodic_return=1\n",
      "global_step=13566, episodic_return=0.99\n",
      "global_step=13581, episodic_return=1\n",
      "global_step=13597, episodic_return=1\n",
      "global_step=13656, episodic_return=1\n",
      "global_step=13699, episodic_return=1\n",
      "global_step=13735, episodic_return=1\n",
      "global_step=13766, episodic_return=1\n",
      "global_step=13852, episodic_return=0.98\n",
      "global_step=13877, episodic_return=1\n",
      "global_step=13922, episodic_return=1\n",
      "global_step=13958, episodic_return=0.95\n",
      "global_step=14037, episodic_return=1\n",
      "global_step=14071, episodic_return=1\n",
      "global_step=14099, episodic_return=1\n",
      "global_step=14125, episodic_return=1\n",
      "global_step=14158, episodic_return=0.98\n",
      "global_step=14189, episodic_return=1\n",
      "global_step=14227, episodic_return=0.98\n",
      "global_step=14264, episodic_return=0.97\n",
      "global_step=14304, episodic_return=0.96\n",
      "global_step=14391, episodic_return=1\n",
      "global_step=14433, episodic_return=1\n",
      "global_step=14507, episodic_return=0.97\n",
      "global_step=14553, episodic_return=0.99\n",
      "global_step=14590, episodic_return=1\n",
      "global_step=14646, episodic_return=1\n",
      "global_step=14718, episodic_return=0.98\n",
      "global_step=14783, episodic_return=1\n",
      "global_step=14829, episodic_return=1\n",
      "global_step=14852, episodic_return=1\n",
      "global_step=14912, episodic_return=1\n",
      "global_step=14966, episodic_return=1\n",
      "global_step=15017, episodic_return=1\n",
      "global_step=15187, episodic_return=0.95\n",
      "global_step=15212, episodic_return=1\n",
      "global_step=15260, episodic_return=1\n",
      "global_step=15302, episodic_return=1\n",
      "global_step=15350, episodic_return=1\n",
      "global_step=15389, episodic_return=1\n",
      "global_step=15441, episodic_return=0.9\n",
      "global_step=15467, episodic_return=0.99\n",
      "global_step=15525, episodic_return=0.98\n",
      "global_step=15550, episodic_return=0.99\n",
      "global_step=15584, episodic_return=1\n",
      "global_step=15685, episodic_return=1\n",
      "global_step=15722, episodic_return=0.98\n",
      "global_step=15735, episodic_return=1\n",
      "global_step=15774, episodic_return=1\n",
      "global_step=15923, episodic_return=0.95\n",
      "global_step=15979, episodic_return=1\n",
      "global_step=16018, episodic_return=0.99\n",
      "global_step=16086, episodic_return=0.99\n",
      "global_step=16113, episodic_return=1\n",
      "global_step=16154, episodic_return=1\n",
      "global_step=16197, episodic_return=0.98\n",
      "global_step=16284, episodic_return=1\n",
      "global_step=16344, episodic_return=1\n",
      "global_step=16422, episodic_return=1\n",
      "global_step=16455, episodic_return=0.99\n",
      "global_step=16494, episodic_return=1\n",
      "global_step=16559, episodic_return=1\n",
      "global_step=16594, episodic_return=1\n",
      "global_step=16621, episodic_return=1\n",
      "global_step=16659, episodic_return=1\n",
      "global_step=16696, episodic_return=1\n",
      "global_step=16761, episodic_return=1\n",
      "global_step=16813, episodic_return=1\n",
      "global_step=16869, episodic_return=1\n",
      "global_step=16903, episodic_return=1\n",
      "global_step=16921, episodic_return=1\n",
      "global_step=16950, episodic_return=1\n",
      "global_step=17009, episodic_return=1\n",
      "global_step=17039, episodic_return=1\n",
      "global_step=17082, episodic_return=1\n",
      "global_step=17120, episodic_return=1\n",
      "global_step=17147, episodic_return=1\n",
      "global_step=17166, episodic_return=1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[217], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 31\u001b[0m         actions \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m         actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mclip(envs\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, envs\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m     33\u001b[0m         hidden_state, next_hidden_state \u001b[38;5;241m=\u001b[39m actor\u001b[38;5;241m.\u001b[39mget_hiddenstate()\n",
      "File \u001b[1;32mc:\\Users\\24401\\anaconda3\\envs\\CBRL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\24401\\anaconda3\\envs\\CBRL\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[215], line 173\u001b[0m, in \u001b[0;36mESNLayer.forward\u001b[1;34m(self, u, hidden_state)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_state \u001b[38;5;241m=\u001b[39m hidden_state\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Append output prediction (linear transformation of the reservoir state)\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_out(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnext_hidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# Stack outputs over time and return\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRY NOT TO MODIFY: start the game\n",
    "obs, _ = envs.reset(seed=args.seed)\n",
    "hidden_state = np.zeros((reservoir_size,))\n",
    "input = np.concat((obs,hidden_state))\n",
    "all_trajectory = {}\n",
    "for global_step in range(args.total_timesteps):\n",
    "\n",
    "    # SAVE MODEL: save model per 2000 global steps\n",
    "    model_dir = f\"runs/{run_name}/models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    if global_step % 2000 == 0 or global_step == args.total_timesteps:\n",
    "        model_path = f\"runs/{run_name}/models/{global_step}steps.cleanrl_model\"\n",
    "        torch.save(actor.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    obs = input[:envs.observation_space.shape[0]]\n",
    "    hidden_state = input[envs.observation_space.shape[0]:]\n",
    "\n",
    "    # INFERENCE:\n",
    "    if global_step % 8000 == 0 or global_step == args.total_timesteps-1:\n",
    "        args.initial_positions = [(2,2), (2,10), (2,18), (10,2), (10,18), (18,2), (18,10), (18,18)]\n",
    "        trajectory = inference(actor, hidden_state, args)\n",
    "        all_trajectory[f'{global_step}'] = trajectory\n",
    "\n",
    "\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    if global_step < args.learning_starts:\n",
    "        actions = envs.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            actions = actor(torch.Tensor(obs).to(device), torch.Tensor(hidden_state).to(device))\n",
    "            actions = actions.cpu().numpy().clip(envs.action_space.low, envs.action_space.high)\n",
    "            hidden_state, next_hidden_state = actor.get_hiddenstate()\n",
    "            hidden_state, next_hidden_state = hidden_state.cpu(), next_hidden_state.cpu()\n",
    "            \n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "    input = input = np.concat((obs,hidden_state))\n",
    "    next_input = np.concat((next_obs, next_hidden_state))\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    if \"final_info\" in infos or terminations:\n",
    "        print(f\"global_step={global_step}, episodic_return={infos['episode']['r']}\")\n",
    "        writer.add_scalar(\"charts/episodic_return\", infos[\"episode\"][\"r\"], global_step)\n",
    "        writer.add_scalar(\"charts/episodic_length\", infos[\"episode\"][\"l\"], global_step)     \n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "    next_obs = next_obs.copy()\n",
    "    if truncations:\n",
    "        real_next_obs = infos[\"final_observation\"]\n",
    "    rb.add(input, next_input, actions, rewards, terminations, infos)\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    input = next_input\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_observations = data.next_observations[:,:envs.observation_space.shape[0]]\n",
    "            next_hidden_states = data.next_observations[:,envs.observation_space.shape[0]:]\n",
    "            observations = data.observations[:,:envs.observation_space.shape[0]]\n",
    "            hidden_states = data.observations[:,envs.observation_space.shape[0]:]\n",
    "            next_state_actions = target_actor(next_observations, next_hidden_states).clamp(\n",
    "                envs.action_space.low[0], envs.action_space.high[0]\n",
    "            )\n",
    "            qf1_next_target = qf1_target(next_observations, next_state_actions)\n",
    "            qf2_next_target = qf2_target(next_observations, next_state_actions)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target)\n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)\n",
    "\n",
    "        qf1_a_values = qf1(observations, data.actions).view(-1)\n",
    "        qf2_a_values = qf2(observations, data.actions).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "        qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        # optimize the model\n",
    "        q_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        if global_step % args.policy_frequency == 0:\n",
    "            actor_loss = -qf1(observations, actor(observations, hidden_states)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            actor.hebbtrace_update(hidden_states)\n",
    "            #actor.hebbian_update(hidden_states)\n",
    "            \n",
    "\n",
    "            # update the target network\n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "envs.close()\n",
    "writer.close()\n",
    "\n",
    "with open(f\"runs/{run_name}/all_trajectory.pkl\", \"wb\") as file:\n",
    "    pickle.dump(all_trajectory, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0000, -0.0000, -0.0000,  ..., -0.1560, -0.0000,  0.1560],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor.hebb_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig_dir = f'runs/{run_name}/figures'\n",
    "os.makedirs(fig_dir, exist_ok=True)\n",
    "for trajectories in all_trajectory:\n",
    "    step = trajectories\n",
    "    trajectories = all_trajectory[f'{trajectories}']\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(trajectories)))\n",
    "    for i, (trajectory, color) in enumerate(zip(trajectories, colors)):\n",
    "        ax.plot(trajectory[:, 0], trajectory[:, 1], color=color)\n",
    "        ax.scatter(trajectory[:, 0], trajectory[:, 1], color=color, s=10)\n",
    "    circle = plt.Circle((10, 10), radius=2, color='black', fill=False, linestyle='-', linewidth=2)\n",
    "    ax.add_patch(circle)\n",
    "    plt.xlim(0, 20)\n",
    "    plt.ylim(0, 20)\n",
    "    plt.xlabel('X Position')\n",
    "    plt.ylabel('Y Position')\n",
    "    plt.title('Agent Trajectory in 2D Environment')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{fig_dir}/{step}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM SEEDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "for i in range(10):\n",
    "    if sb3.__version__ < \"2.0\":\n",
    "        raise ValueError(\n",
    "                \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
    "    poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
    "    \"\"\"\n",
    "            )\n",
    "\n",
    "    args = Args()\n",
    "    args.exp_name = 'HebbianSpectral'\n",
    "    args.seed = random.randint(0, 2**32 - 1)\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    "    )\n",
    "\n",
    "    # TRY NOT TO MODIFY: seeding\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "\n",
    "    # env setup\n",
    "    torch.manual_seed(args.seed)\n",
    "    envs = make_env(args.env_id, args.seed, 0, args.capture_video, run_name)()\n",
    "    reservoir_size = 256\n",
    "    input_space = spaces.Box(low=-1, high=1, shape=(envs.observation_space.shape[0]+reservoir_size,), dtype=np.float32)\n",
    "\n",
    "    actor =  ESNLayer(envs, 256).to(device)\n",
    "    qf1 = QNetwork(envs).to(device)\n",
    "    qf2 = QNetwork(envs).to(device)\n",
    "    qf1_target = QNetwork(envs).to(device)\n",
    "    qf2_target = QNetwork(envs).to(device)\n",
    "    target_actor = ESNLayer(envs, 256).to(device)\n",
    "    target_actor.load_state_dict(actor.state_dict())\n",
    "    qf1_target.load_state_dict(qf1.state_dict())\n",
    "    qf2_target.load_state_dict(qf2.state_dict())\n",
    "    q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.learning_rate)\n",
    "    actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.learning_rate)\n",
    "\n",
    "    envs.observation_space.dtype = np.float32\n",
    "\n",
    "\n",
    "\n",
    "    rb = ReplayBuffer(\n",
    "        args.buffer_size,\n",
    "        input_space,\n",
    "        envs.action_space,\n",
    "        device,\n",
    "        handle_timeout_termination=False,\n",
    "    )\n",
    "    start_time = time.time()\n",
    "\n",
    "    # TRY NOT TO MODIFY: start the game\n",
    "    obs, _ = envs.reset(seed=args.seed)\n",
    "    hidden_state = np.zeros((reservoir_size,))\n",
    "    input = np.concat((obs,hidden_state))\n",
    "    all_trajectory = {}\n",
    "    for global_step in range(args.total_timesteps):\n",
    "\n",
    "        # SAVE MODEL: save model per 2000 global steps\n",
    "        model_dir = f\"runs/{run_name}/models\"\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        if global_step % 2000 == 0 or global_step == args.total_timesteps:\n",
    "            model_path = f\"runs/{run_name}/models/{global_step}steps.cleanrl_model\"\n",
    "            torch.save(actor.state_dict(), model_path)\n",
    "\n",
    "\n",
    "        obs = input[:envs.observation_space.shape[0]]\n",
    "        hidden_state = input[envs.observation_space.shape[0]:]\n",
    "\n",
    "        # INFERENCE:\n",
    "        if global_step % 8000 == 0 or global_step == args.total_timesteps-1:\n",
    "            args.initial_positions = [(2,2), (2,10), (2,18), (10,2), (10,18), (18,2), (18,10), (18,18)]\n",
    "            trajectory = inference(actor, hidden_state, args)\n",
    "            all_trajectory[f'{global_step}'] = trajectory\n",
    "\n",
    "\n",
    "        # ALGO LOGIC: put action logic here\n",
    "        if global_step < args.learning_starts:\n",
    "            actions = envs.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                actions = actor(torch.Tensor(obs).to(device), torch.Tensor(hidden_state).to(device))\n",
    "                actions = actions.cpu().numpy().clip(envs.action_space.low, envs.action_space.high)\n",
    "                hidden_state, next_hidden_state = actor.get_hiddenstate()\n",
    "                hidden_state, next_hidden_state = hidden_state.cpu(), next_hidden_state.cpu()\n",
    "                \n",
    "\n",
    "        # TRY NOT TO MODIFY: execute the game and log data.\n",
    "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "        input = input = np.concat((obs,hidden_state))\n",
    "        next_input = np.concat((next_obs, next_hidden_state))\n",
    "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "        if \"final_info\" in infos or terminations:\n",
    "            writer.add_scalar(\"charts/episodic_return\", infos[\"episode\"][\"r\"], global_step)\n",
    "            writer.add_scalar(\"charts/episodic_length\", infos[\"episode\"][\"l\"], global_step)     \n",
    "\n",
    "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "        next_obs = next_obs.copy()\n",
    "        if truncations:\n",
    "            real_next_obs = infos[\"final_observation\"]\n",
    "        rb.add(input, next_input, actions, rewards, terminations, infos)\n",
    "\n",
    "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "        input = next_input\n",
    "\n",
    "        # ALGO LOGIC: training.\n",
    "        if global_step > args.learning_starts:\n",
    "            data = rb.sample(args.batch_size)\n",
    "            with torch.no_grad():\n",
    "                next_observations = data.next_observations[:,:envs.observation_space.shape[0]]\n",
    "                next_hidden_states = data.next_observations[:,envs.observation_space.shape[0]:]\n",
    "                observations = data.observations[:,:envs.observation_space.shape[0]]\n",
    "                hidden_states = data.observations[:,envs.observation_space.shape[0]:]\n",
    "                next_state_actions = target_actor(next_observations, next_hidden_states).clamp(\n",
    "                    envs.action_space.low[0], envs.action_space.high[0]\n",
    "                )\n",
    "                qf1_next_target = qf1_target(next_observations, next_state_actions)\n",
    "                qf2_next_target = qf2_target(next_observations, next_state_actions)\n",
    "                min_qf_next_target = torch.min(qf1_next_target, qf2_next_target)\n",
    "                next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)\n",
    "\n",
    "            qf1_a_values = qf1(observations, data.actions).view(-1)\n",
    "            qf2_a_values = qf2(observations, data.actions).view(-1)\n",
    "            qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "            qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "            qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "            # optimize the model\n",
    "            q_optimizer.zero_grad()\n",
    "            qf_loss.backward()\n",
    "            q_optimizer.step()\n",
    "\n",
    "            if global_step % args.policy_frequency == 0:\n",
    "                actor_loss = -qf1(observations, actor(observations, hidden_states)).mean()\n",
    "                actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                actor_optimizer.step()\n",
    "                actor.hebbtrace_update(hidden_states)\n",
    "                \n",
    "\n",
    "                # update the target network\n",
    "                for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "                for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "                for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                    target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "            if global_step % 100 == 0:\n",
    "                writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "                writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "                writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "    with open(f\"runs/{run_name}/all_trajectory.pkl\", \"wb\") as file:\n",
    "        pickle.dump(all_trajectory, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
