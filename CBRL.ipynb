{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tyro\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from goal_task import GoalTask\n",
    "\n",
    "@dataclass\n",
    "class Args:\n",
    "    exp_name: str = 'MoA_ESN'\n",
    "    \"\"\"the name of this experiment\"\"\"\n",
    "    seed: int = 1\n",
    "    \"\"\"seed of the experiment\"\"\"\n",
    "    torch_deterministic: bool = True\n",
    "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
    "    cuda: bool = True\n",
    "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
    "    track: bool = False\n",
    "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
    "    wandb_project_name: str = \"MoA_ESN_wandb\"\n",
    "    \"\"\"the wandb's project name\"\"\"\n",
    "    wandb_entity: str = None\n",
    "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
    "    capture_video: bool = False\n",
    "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
    "    save_model: bool = False\n",
    "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
    "    upload_model: bool = False\n",
    "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
    "    hf_entity: str = \"\"\n",
    "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    env_id: str = \"GoalTask\"\n",
    "    \"\"\"the id of the environment\"\"\"\n",
    "    total_timesteps: int = 40000\n",
    "    \"\"\"total timesteps of the experiments\"\"\"\n",
    "    learning_rate: float = 3e-4\n",
    "    \"\"\"the learning rate of the optimizer\"\"\"\n",
    "    buffer_size: int = 64\n",
    "    \"\"\"the replay memory buffer size\"\"\"\n",
    "    gamma: float = 0.95\n",
    "    \"\"\"the discount factor gamma\"\"\"\n",
    "    tau: float = 0.05\n",
    "    \"\"\"target smoothing coefficient (default: 0.005)\"\"\"\n",
    "    batch_size: int = 64\n",
    "    \"\"\"the batch size of sample from the reply memory\"\"\"\n",
    "    policy_noise: float = 0.2\n",
    "    \"\"\"the scale of policy noise\"\"\"\n",
    "    exploration_noise: float = 0.1\n",
    "    \"\"\"the scale of exploration noise\"\"\"\n",
    "    learning_starts: int = 0\n",
    "    \"\"\"timestep to start learning\"\"\"\n",
    "    policy_frequency: int = 2\n",
    "    \"\"\"the frequency of training policy (delayed)\"\"\"\n",
    "    noise_clip: float = 0.5\n",
    "    \"\"\"noise clip parameter of the Target Policy Smoothing Regularization\"\"\"\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        if capture_video and idx == 0:\n",
    "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
    "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        else:\n",
    "            env = GoalTask()\n",
    "        env.action_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "# ALGO LOGIC: initialize agent here:\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod() + np.prod(env.action_space.shape), 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        x = torch.cat([x, a], 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(np.array(env.observation_space.shape).prod(), 32)\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.fc_mu = nn.Linear(32, np.prod(env.action_space.shape))\n",
    "        # action rescaling\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((env.action_space.high - env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((env.action_space.high + env.action_space.low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc_mu(x))\n",
    "        return x * self.action_scale + self.action_bias\n",
    "\n",
    "\n",
    "class ESNLayer(nn.Module):\n",
    "    def __init__(self, env, reservoir_size, spectral_radius=0.95, g=2.2, sparsity=0.1, device='cuda'):\n",
    "        super(ESNLayer, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.input_size = np.array(env.observation_space.shape).prod()\n",
    "        self.reservoir_size = reservoir_size\n",
    "        self.spectral_radius = spectral_radius\n",
    "        self.g = g\n",
    "        \n",
    "        # Input weights: weights are sampled from a uniform distribution over [-0.5,0.5]\n",
    "        self.W_in = torch.rand(reservoir_size, self.input_size, device=self.device) - 0.5\n",
    "        \n",
    "        # Reservoir weights: sparse random matrix\n",
    "        self.W = torch.randn(reservoir_size, reservoir_size, device=self.device)\n",
    "        self.W[torch.rand(reservoir_size, reservoir_size) > sparsity] = 0.0  # Sparsify\n",
    "        \n",
    "        # Scale the reservoir weights to have the desired spectral radius\n",
    "        _, eigenvalues = torch.linalg.eig(self.W)\n",
    "        max_eigenvalue = torch.max(torch.abs(eigenvalues))\n",
    "        self.W *= self.spectral_radius / max_eigenvalue\n",
    "        \n",
    "        # The output weights, to be learned during training\n",
    "        self.W_out = nn.Linear((reservoir_size+self.input_size), 1, device=self.device)\n",
    "        \n",
    "    \n",
    "    def forward(self, u, hidden_state):\n",
    "        # Compute the new reservoir state with leaky integration\n",
    "        with torch.no_grad():\n",
    "            print(u.shape, hidden_state.shape)\n",
    "            next_hidden_state = torch.matmul(self.W_in, u.T) + self.g * torch.matmul(self.W, hidden_state.T)\n",
    "            next_hidden_state = torch.tanh(next_hidden_state)\n",
    "        self.next_hidden_state = next_hidden_state\n",
    "        self.hidden_state = hidden_state\n",
    "        # Append output prediction (linear transformation of the reservoir state)\n",
    "        output = torch.tanh(self.W_out(torch.concat([next_hidden_state.T, u.T])))\n",
    "\n",
    "        # Stack outputs over time and return\n",
    "        return output\n",
    "    \n",
    "    def get_hiddenstate(self):\n",
    "        return self.hidden_state, self.next_hidden_state\n",
    "    \n",
    "    def reset_state(self):\n",
    "        \"\"\"Reset the hidden state of the ESN.\"\"\"\n",
    "        self.hiddenstate = torch.zeros(self.reservoir_size)\n",
    "\n",
    "\n",
    "\n",
    "class ESNReplayBuffer:\n",
    "    def __init__(self, max_size):\n",
    "        self.max_size = max_size\n",
    "        self.buffer = []\n",
    "    \n",
    "    def add(self, experience):\n",
    "        \"\"\"Add experience to the buffer\"\"\"\n",
    "        if len(self.buffer) >= self.max_size:\n",
    "            self.buffer.pop(0)  # Remove the oldest experience if full\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences from the buffer\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        return map(np.array, zip(*batch))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "\n",
    "if sb3.__version__ < \"2.0\":\n",
    "    raise ValueError(\n",
    "            \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
    "poetry run pip install \"stable_baselines3==2.0.0a1\"\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "args = Args()\n",
    "run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "if args.track:\n",
    "    import wandb\n",
    "\n",
    "    wandb.init(\n",
    "        project=args.wandb_project_name,\n",
    "        entity=args.wandb_entity,\n",
    "        sync_tensorboard=True,\n",
    "        config=vars(args),\n",
    "        name=run_name,\n",
    "        monitor_gym=True,\n",
    "        save_code=True,\n",
    "    )\n",
    "writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "writer.add_text(\n",
    "    \"hyperparameters\",\n",
    "    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
    ")\n",
    "\n",
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env setup\n",
    "envs = make_env(args.env_id, args.seed, 0, args.capture_video, run_name)()\n",
    "\n",
    "\n",
    "actor =  ESNLayer(envs, 256).to(device)\n",
    "qf1 = QNetwork(envs).to(device)\n",
    "qf2 = QNetwork(envs).to(device)\n",
    "qf1_target = QNetwork(envs).to(device)\n",
    "qf2_target = QNetwork(envs).to(device)\n",
    "target_actor = ESNLayer(envs, 256).to(device)\n",
    "target_actor.load_state_dict(actor.state_dict())\n",
    "qf1_target.load_state_dict(qf1.state_dict())\n",
    "qf2_target.load_state_dict(qf2.state_dict())\n",
    "q_optimizer = optim.Adam(list(qf1.parameters()) + list(qf2.parameters()), lr=args.learning_rate)\n",
    "actor_optimizer = optim.Adam(list(actor.parameters()), lr=args.learning_rate)\n",
    "\n",
    "envs.observation_space.dtype = np.float32\n",
    "\n",
    "\n",
    "\n",
    "rb = ESNReplayBuffer(args.buffer_size)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5]) torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "obs, _ = envs.reset(seed=args.seed)\n",
    "actions = actor(torch.Tensor(obs).to(device), torch.Tensor(np.zeros((256))).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.6)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - np.linalg.norm(np.array([2,18]) - np.array([10,10]))/np.linalg.norm(np.array([0,0])-np.array([20,20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY NOT TO MODIFY: start the game\n",
    "obs, _ = envs.reset(seed=args.seed)\n",
    "for global_step in range(args.total_timesteps):\n",
    "    # ALGO LOGIC: put action logic here\n",
    "    if global_step < args.learning_starts:\n",
    "        actions = envs.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            actions = actor(torch.Tensor(obs).to(device))\n",
    "            actions += torch.normal(0, actor.action_scale * args.exploration_noise)\n",
    "            actions = actions.cpu().numpy().clip(envs.action_space.low, envs.action_space.high)\n",
    "\n",
    "    # TRY NOT TO MODIFY: execute the game and log data.\n",
    "    next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
    "\n",
    "    # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
    "    if \"final_info\" in infos or terminations:\n",
    "        print(f\"global_step={global_step}, episodic_return={infos['episode']['r']}\")\n",
    "        writer.add_scalar(\"charts/episodic_return\", infos[\"episode\"][\"r\"], global_step)\n",
    "        writer.add_scalar(\"charts/episodic_length\", infos[\"episode\"][\"l\"], global_step)     \n",
    "\n",
    "    # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
    "    real_next_obs = next_obs.copy()\n",
    "    if truncations:\n",
    "        real_next_obs = infos[\"final_observation\"]\n",
    "    rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
    "\n",
    "    # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
    "    obs = next_obs\n",
    "\n",
    "    # ALGO LOGIC: training.\n",
    "    if global_step > args.learning_starts:\n",
    "        data = rb.sample(args.batch_size)\n",
    "        with torch.no_grad():\n",
    "            next_state_actions = target_actor(data.next_observations).clamp(\n",
    "                envs.action_space.low[0], envs.action_space.high[0]\n",
    "            )\n",
    "            qf1_next_target = qf1_target(data.next_observations, next_state_actions)\n",
    "            qf2_next_target = qf2_target(data.next_observations, next_state_actions)\n",
    "            min_qf_next_target = torch.min(qf1_next_target, qf2_next_target)\n",
    "            next_q_value = data.rewards.flatten() + (1 - data.dones.flatten()) * args.gamma * (min_qf_next_target).view(-1)\n",
    "\n",
    "        qf1_a_values = qf1(data.observations, data.actions).view(-1)\n",
    "        qf2_a_values = qf2(data.observations, data.actions).view(-1)\n",
    "        qf1_loss = F.mse_loss(qf1_a_values, next_q_value)\n",
    "        qf2_loss = F.mse_loss(qf2_a_values, next_q_value)\n",
    "        qf_loss = qf1_loss + qf2_loss\n",
    "\n",
    "        # optimize the model\n",
    "        q_optimizer.zero_grad()\n",
    "        qf_loss.backward()\n",
    "        q_optimizer.step()\n",
    "\n",
    "        if global_step % args.policy_frequency == 0:\n",
    "            actor_loss = -qf1(data.observations, actor(data.observations)).mean()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "\n",
    "            # update the target network\n",
    "            for param, target_param in zip(actor.parameters(), target_actor.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf1.parameters(), qf1_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "            for param, target_param in zip(qf2.parameters(), qf2_target.parameters()):\n",
    "                target_param.data.copy_(args.tau * param.data + (1 - args.tau) * target_param.data)\n",
    "\n",
    "        if global_step % 100 == 0:\n",
    "            writer.add_scalar(\"losses/qf1_values\", qf1_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_values\", qf2_a_values.mean().item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf1_loss\", qf1_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf2_loss\", qf2_loss.item(), global_step)\n",
    "            writer.add_scalar(\"losses/qf_loss\", qf_loss.item() / 2.0, global_step)\n",
    "            writer.add_scalar(\"losses/actor_loss\", actor_loss.item(), global_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CBRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
